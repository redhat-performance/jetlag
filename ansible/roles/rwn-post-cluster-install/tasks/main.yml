---
# rwn-post-cluster-install tasks

- name: Create directories for rwn cluster configuration items
  file:
    path: "{{ item }}"
    state: directory
  with_items:
    - "{{ bastion_cluster_config_dir }}"
    - "{{ bastion_cluster_config_dir }}/livecd"
    - "{{ bastion_cluster_config_dir }}/autorules"
    - "{{ bastion_cluster_config_dir }}/mcp"
    - "{{ bastion_cluster_config_dir }}/localstorage"
    - "{{ bastion_cluster_config_dir }}/openshift-monitoring"

- name: Get credentials (kubeconfig and kubeadmin password) from installed cluster
  get_url:
    url: "http://{{ assisted_installer_host }}:{{ assisted_installer_port }}/api/assisted-install/v1/clusters/{{ ai_cluster_id }}/downloads/files?file_name={{ item }}"
    dest: "{{ bastion_cluster_config_dir }}/{{ item }}"
  with_items:
    - kubeadmin-password
    - kubeconfig

- name: Place templated configuration items
  template:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
  with_items:
    - src: autorules.yml.j2
      dest: "{{ bastion_cluster_config_dir }}/autorules/autorules.yml"
    - src: localstorage.yml.j2
      dest: "{{ bastion_cluster_config_dir }}/localstorage/localstorage.yml"
    - src: localvolume.yml.j2
      dest: "{{ bastion_cluster_config_dir }}/localstorage/localvolume.yml"
    - src: cluster-monitoring-config.yml.j2
      dest: "{{ bastion_cluster_config_dir }}/openshift-monitoring/cluster-monitoring-config.yml"

- name: Create MachineConfigPool manifest per Remote Worker Node
  template:
    src: mcp.yml.j2
    dest: "{{ bastion_cluster_config_dir }}/mcp/mcp-rwn-{{ hostvars[item].vlan }}.yml"
  with_items:
    - "{{ groups['remoteworker'] }}"

# Witnessed at least once where the cluster did not seem to be ready for commands
# thus adding retries to this first command
- name: Apply MachineConfigPool manifests
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc apply -f {{ bastion_cluster_config_dir }}/mcp/
  register: oc_result
  until: oc_result.rc != 1
  retries: 60
  delay: 2

- name: Clone performance-dashboards
  git:
    repo: https://github.com/cloud-bulldozer/performance-dashboards.git
    dest: "{{ bastion_cluster_config_dir }}/performance-dashboards"
    force: true
    version: master

- name: Deploy performance-dashboards
  shell: |
    export KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig
    {{ bastion_cluster_config_dir }}/performance-dashboards/dittybopper/deploy.sh
  args:
    chdir: "{{ bastion_cluster_config_dir }}/performance-dashboards/dittybopper/"
  when: setup_performance_dashboards | default(true) | bool

- name: Deploy autorules
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc apply -f {{ bastion_cluster_config_dir }}/autorules/autorules.yml

- name: Ensure control-plane nodes are schedulable
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc patch scheduler cluster --type merge -p='{"spec": {"mastersSchedulable": true }}'

- name: Move ingress controllers to control plane nodes and scale up to have 1 per control-plane node
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec": {"nodePlacement": {"nodeSelector": {"matchLabels": {"node-role.kubernetes.io/master": "" }}}, "replicas": 3 }}' --type=merge

# This prevents localstorage from being deployed thus I have commented it out
# - name: Disable default OperatorHub sources
#   shell: |
#     KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc patch OperatorHub cluster --type json -p '[{"op": "add", "path": "/spec/disableAllDefaultSources", "value": true}]'

- name: Move internal image registry to control plane
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc patch configs.imageregistry.operator.openshift.io/cluster --type merge  -p='{"spec": {"nodeSelector": {"node-role.kubernetes.io/master": "" }}}'

- name: Remove internal Image registry
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc patch configs.imageregistry.operator.openshift.io/cluster --type merge  -p='{"spec": {"managementState": "Removed" }}'

- name: Add kube-burner sa
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc create sa kubeburner
  when: setup_kube_burner_sa | default(true) | bool

- name: Add cluster-admin role to kube-burner sa
  shell: |
    KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc adm policy add-cluster-role-to-user -z kubeburner cluster-admin
  when: setup_kube_burner_sa | default(true) | bool

- name: Setup control-plane nodes localstorage
  when: controlplane_localstorage_configuration
  block:
  - name: Label the control-plane nodes
    shell: |
      KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc label no --overwrite {{ item }} localstorage=true prometheus=true
    with_items: "{{ groups['controlplane'] }}"

  - name: Install local-storage operator
    shell:
      KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc apply -f {{ bastion_cluster_config_dir }}/localstorage/localstorage.yml

  # The localvolume resource will not be instantly available, thus retry for around 2 minutes
  - name: Create localvolume resource
    shell:
      KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc apply -f {{ bastion_cluster_config_dir }}/localstorage/localvolume.yml
    register: lv_result
    until: not lv_result.failed
    retries: 60
    delay: 2

  - name: Configure OpenShift-Monitoring Prometheus to use localstorage
    shell:
      KUBECONFIG={{ bastion_cluster_config_dir }}/kubeconfig oc apply -f {{ bastion_cluster_config_dir }}/openshift-monitoring/cluster-monitoring-config.yml

# Ideally we could check the status for the MCO to see that the latest machineconfigs
# have been rendered into the "rendered" config
- name: Pause for 30s to allow MCO to render latest MachineConfig
  pause:
    seconds: 30
